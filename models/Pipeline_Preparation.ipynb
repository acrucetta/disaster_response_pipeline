{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Pipeline Preparation.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnNjYV1_dIdF",
        "colab_type": "text"
      },
      "source": [
        "# ETL Pipeline Preparation\n",
        "Follow the instructions below to help you create your ETL pipeline.\n",
        "### 1. Import libraries and load datasets.\n",
        "- Import Python libraries\n",
        "- Load `messages.csv` into a dataframe and inspect the first few lines.\n",
        "- Load `categories.csv` into a dataframe and inspect the first few lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi-esJRqdIdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeJQCJvatbRy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s git://github.com/acrucetta/disaster_response_pipeline.git cloned-repo\n",
        "%cd cloned-repo\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7WQ8RQLdIeO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pwd\n",
        "%cd data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsKBC5mgdIef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load messages dataset\n",
        "messages = pd.read_csv('/content/cloned-repo/data/disaster_messages.csv')\n",
        "messages.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKhal6F5dIek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load categories dataset\n",
        "categories = pd.read_csv('/content/cloned-repo/data/disaster_categories.csv')\n",
        "categories.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZ6tQRzXdIev",
        "colab_type": "text"
      },
      "source": [
        "### 2. Merge datasets.\n",
        "- Merge the messages and categories datasets using the common id\n",
        "- Assign this combined dataset to `df`, which will be cleaned in the following steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpNmB9YzdIew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# merge datasets\n",
        "df = pd.merge(messages,categories,how = 'inner', left_on = \"id\", right_on = \"id\")\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNTMeTYDdIe4",
        "colab_type": "text"
      },
      "source": [
        "### 3. Split `categories` into separate category columns.\n",
        "- Split the values in the `categories` column on the `;` character so that each value becomes a separate column. You'll find [this method](https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.Series.str.split.html) very helpful! Make sure to set `expand=True`.\n",
        "- Use the first row of categories dataframe to create column names for the categories data.\n",
        "- Rename columns of `categories` with new column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boM-KHn5dIe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe of the 36 individual category columns\n",
        "categories = df.categories.str.split(\";\",expand = True)\n",
        "categories.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW_Nf_bldIe-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select the first row of the categories dataframe\n",
        "row = categories.iloc[0]\n",
        "\n",
        "# use this row to extract a list of new column names for categories.\n",
        "# one way is to apply a lambda function that takes everything \n",
        "# up to the second to last character of each string with slicing\n",
        "category_colnames = row.str.slice(stop= -2)\n",
        "print(category_colnames)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt-rpx_KdIfD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# rename the columns of `categories`\n",
        "categories.columns = category_colnames\n",
        "categories.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK0MCG6VdIfK",
        "colab_type": "text"
      },
      "source": [
        "### 4. Convert category values to just numbers 0 or 1.\n",
        "- Iterate through the category columns in df to keep only the last character of each string (the 1 or 0). For example, `related-0` becomes `0`, `related-1` becomes `1`. Convert the string to a numeric value.\n",
        "- You can perform [normal string actions on Pandas Series](https://pandas.pydata.org/pandas-docs/stable/text.html#indexing-with-str), like indexing, by including `.str` after the Series. You may need to first convert the Series to be of type string, which you can do with `astype(str)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT-PLxP3dIfK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in categories:\n",
        "    # set each value to be the last character of the string\n",
        "    categories[column] = categories[column].str.slice(start = -1)\n",
        "    \n",
        "    # convert column from string to numeric\n",
        "    categories[column] = pd.to_numeric(categories[column])\n",
        "    \n",
        "categories.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ilf3JGPdIfR",
        "colab_type": "text"
      },
      "source": [
        "### 5. Replace `categories` column in `df` with new category columns.\n",
        "- Drop the categories column from the df dataframe since it is no longer needed.\n",
        "- Concatenate df and categories data frames."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsT8EnEWdIfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAb6VBTrdIfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop the original categories column from `df`\n",
        "df = df.drop(columns = ['categories'])\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykg_aioUdIfi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenate the original dataframe with the new `categories` dataframe\n",
        "df = pd.concat([df,categories], axis=1)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqEwsIdLdIfp",
        "colab_type": "text"
      },
      "source": [
        "### 6. Remove duplicates.\n",
        "- Check how many duplicates are in this dataset.\n",
        "- Drop the duplicates.\n",
        "- Confirm duplicates were removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj9hzdopdIfp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check number of duplicates\n",
        "df.duplicated().value_counts()\n",
        "df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2AJbE6DdIft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# drop duplicates\n",
        "df = df.drop_duplicates()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GidpzTXjdIfx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check number of duplicates\n",
        "df.duplicated().value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V5TPMDidIf2",
        "colab_type": "text"
      },
      "source": [
        "### 7. Save the clean dataset into an sqlite database.\n",
        "You can do this with pandas [`to_sql` method](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html) combined with the SQLAlchemy library. Remember to import SQLAlchemy's `create_engine` in the first cell of this notebook to use it below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4z0t-uWdIf3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "engine = create_engine('sqlite:///disaster_response.db')\n",
        "df.to_sql('messages_cat', engine, index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpdG-Pz0dIf_",
        "colab_type": "text"
      },
      "source": [
        "### 8. Use this notebook to complete `etl_pipeline.py`\n",
        "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database based on new datasets specified by the user. Alternatively, you can complete `etl_pipeline.py` in the classroom on the `Project Workspace IDE` coming later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQAS4r_0dIgA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_sql_table(\"messages_cat\",engine)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOB5R-lb49XJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.message[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d46jJLVndIgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kcU7XYzdIgL",
        "colab_type": "text"
      },
      "source": [
        "# ML Pipeline Preparation\n",
        "Follow the instructions below to help you create your ML pipeline.\n",
        "### Importing libraries and load data from database.\n",
        "- Import Python libraries\n",
        "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
        "- Define feature and target variables X and Y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLtDuAgedIgM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import plot_roc_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pickle\n",
        "\n",
        "import nltk\n",
        "nltk.download(['punkt', 'wordnet','stopwords','averaged_perceptron_tagger'])\n",
        "\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8Mz7pnudIgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_sql_table('messages_cat', engine)\n",
        "X = df['message']\n",
        "y = df[category_colnames_clean]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oevSXP8QZln2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "category_colnames_clean = category_colnames.drop(labels = [0,35,34,27,18, 28])\n",
        "category_colnames_clean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcV8WrOnA314",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropping abstract categories from Y to ease analyis\n",
        "#y = y.drop(columns= [\"related\",\"other_infrastructure\",\"other_weather\",\"other_aid\",\"direct_report\",\"weather_related\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g09xnArOaoXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiOtDFMsdIgW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words(\"english\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7tz-OD5dIgc",
        "colab_type": "text"
      },
      "source": [
        "### Writing a tokenization function to process our text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y9TjdwIdIgd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(text):\n",
        "    \"\"\"\n",
        "    Normalize, tokenize and stems texts.\n",
        "    \n",
        "    Input:\n",
        "    text: string. Sentence containing a message.\n",
        "    \n",
        "    Output:\n",
        "    stemmed_tokens: list of strings. A list of strings containing normalized and stemmed tokens.\n",
        "    \"\"\"\n",
        "    url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
        "    \n",
        "    # get list of all urls using regex\n",
        "    detected_urls = re.findall(url_regex, text) \n",
        "    \n",
        "    # replace each url in text string with placeholder\n",
        "    for url in detected_urls:\n",
        "        text = text.replace(url, \"urlplaceholder\")\n",
        "\n",
        "    # tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # initiate lemmatizer\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # iterate through each token\n",
        "    clean_tokens = []\n",
        "    for tok in tokens:\n",
        "        \n",
        "        # lemmatize, normalize case, and remove leading/trailing white space\n",
        "        clean_tok = lemmatizer.lemmatize(tok).lower().strip()\n",
        "        clean_tokens.append(clean_tok)\n",
        "\n",
        "    return clean_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdMnKrKmdIgj",
        "colab_type": "text"
      },
      "source": [
        "### 3. Pipeline #1 - K Nearest Neighbors [Default]\n",
        "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr5RG1MadIgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_pipeline():\n",
        "    pipeline = Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer()),\n",
        "                ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
        "                ])\n",
        "    return pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYHkjzHRdIhl",
        "colab_type": "text"
      },
      "source": [
        "#### Train pipeline\n",
        "- Split data into train and test sets\n",
        "- Train pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x66s_9MBdIhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_v1 = model_pipeline()\n",
        "model_v1.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytFJLLFudIhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model_v1.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCkBvuBydIiQ",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline #2 - KNN With Grid Search CV\n",
        "Use grid search to find better parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kR91UXUVdIib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_pipeline_with_cv():\n",
        "    pipeline = Pipeline([\n",
        "        ('text_pipeline', Pipeline([\n",
        "            ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "            ('tfidf', TfidfTransformer())])),\n",
        "        ('clf', MultiOutputClassifier(KNeighborsClassifier()))\n",
        "    ])\n",
        "    \n",
        "    parameters = {\n",
        "        'text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
        "        'clf__estimator__n_neighbors': (10,15)\n",
        "    }\n",
        "         \n",
        "    cv = GridSearchCV(pipeline, param_grid=parameters, n_jobs = 4, verbose = 2)\n",
        "    \n",
        "    return cv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX100krxdIil",
        "colab_type": "text"
      },
      "source": [
        "#### Train pipeline\n",
        "Show the accuracy, precision, and recall of the tuned model.  \n",
        "\n",
        "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhYKWA1tdIim",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_v2 = model_pipeline_with_cv()\n",
        "model_v2.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qru-2fidIis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_v2 = model_v2.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGPaCivkh0DJ",
        "colab_type": "text"
      },
      "source": [
        "### Pipeline v3 - Random Forest Classifier with Grid Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7NaIPkM5DWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StartVerbExtractor(BaseEstimator, TransformerMixin):\n",
        "    def start_verb(self, text):\n",
        "        sentence_list = nltk.sent_tokenize(text)\n",
        "        for sentence in sentence_list:\n",
        "            pos_tags = nltk.pos_tag(tokenize(sentence))\n",
        "            if len(pos_tags) != 0:\n",
        "                first_word, first_tag = pos_tags[0]\n",
        "                if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
        "                    return 1\n",
        "        return 0\n",
        "\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    \n",
        "\n",
        "    def transform(self, X):\n",
        "        X_tag = pd.Series(X).apply(self.start_verb)\n",
        "        return pd.DataFrame(X_tag)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvwEQVIUkJO3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model_v3():\n",
        "    pipeline = Pipeline([\n",
        "        ('features', FeatureUnion([\n",
        "            ('text_pipeline', Pipeline([\n",
        "                ('vect', CountVectorizer(tokenizer=tokenize)),\n",
        "                ('tfidf', TfidfTransformer())\n",
        "            ])),\n",
        "            ('starting_verb', StartVerbExtractor())\n",
        "        ])),\n",
        "        ('clf', MultiOutputClassifier(RandomForestClassifier(n_estimators=100)))\n",
        "    ])\n",
        "    return pipeline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjpdMN7TlXW9",
        "colab_type": "text"
      },
      "source": [
        "#### Train pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLrlyDfGkUiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "build_model_v3().get_params()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMD4HTdQkxlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_v3 = build_model_v3()\n",
        "model_v3.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ft_ZcN04k5wD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred_v3 = model_v3.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDFmLvfldT0",
        "colab_type": "text"
      },
      "source": [
        "### Testing pipelines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vye0gbc8k7XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('KNN Without Grid Search CV')\n",
        "#print(classification_report(y_test, y_pred, target_names = category_colnames))\n",
        "\n",
        "print('KNN With Grid Search CV')\n",
        "#print(classification_report(y_test, y_pred_v2, target_names = category_colnames))\n",
        "\n",
        "print('Random Forest with Starting Verb Extractor')\n",
        "print(classification_report(y_test, y_pred_v3, target_names = category_colnames_clean))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2mZw5-ddIjV",
        "colab_type": "text"
      },
      "source": [
        "### Exporting the model as a pickle file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJyN77uJdIja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib \n",
        "  \n",
        "# Save the model as a pickle in a file \n",
        "joblib.dump(model_v3, 'classifier.pkl') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g34HzmOwKlnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_sql_table('messages_cat', engine)\n",
        "X = df['message']\n",
        "y = df.drop(['message', 'genre', 'id', 'original'], axis=1)\n",
        "y = y.drop(columns=[\"related\", \"other_infrastructure\", \"other_weather\", \"other_aid\", \"direct_report\", \"weather_related\"])\n",
        "category_names = y.columns.tolist()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Sz_3CfhK31b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpDNV_0KGgGh",
        "colab_type": "text"
      },
      "source": [
        "## Visualizations for web application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQyHnjJGsKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from plotly.graph_objs import Bar\n",
        "import json\n",
        "import plotly\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import requests\n",
        "import plotly.graph_objects as go"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3Iq2DL2pEuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "    # extract data needed for visuals\n",
        "    genre_counts = df.groupby('genre').count()['message']\n",
        "    genre_names = list(genre_counts.index)\n",
        "    category_df = df.drop(columns=['id', 'message', 'genre'])\n",
        "    category_sums = pd.DataFrame(np.sum(category_df), columns=[\"count\"]).sort_values(['count'], ascending=False)\n",
        "\n",
        "    # create visuals\n",
        "    graphs = [\n",
        "        {\n",
        "            'data': [\n",
        "                Bar(\n",
        "                    x=genre_counts,\n",
        "                    y=genre_names,\n",
        "                    orientation = 'h'\n",
        "                )\n",
        "            ],\n",
        "            'layout': {\n",
        "                'title': 'Distribution of Message Genres',\n",
        "                'xaxis': {\n",
        "                    'title': \"Count\"\n",
        "                },\n",
        "                'yaxis': {\n",
        "                    'title': \"Genre\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            'data': [\n",
        "                    Bar(\n",
        "                        y= category_sums.index,\n",
        "                        x=category_sums['count'],\n",
        "                        orientation = 'h'\n",
        "                    )\n",
        "                ],\n",
        "            'layout': {\n",
        "                'title': \"Distribution of Message Types\",\n",
        "                \"xaxis\": {\n",
        "                    'title':'Count'\n",
        "                },\n",
        "                'yaxis': {\n",
        "                    'title':'Message Type'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # encode plotly graphs in JSON\n",
        "    ids = [\"graph-{}\".format(i) for i, _ in enumerate(graphs)]\n",
        "    graphJSON = json.dumps(graphs, cls=plotly.utils.PlotlyJSONEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwIZVXi55buh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}